name: zero_shot_verifier_fewshot_rlm
metadata:
  type: VerifierGraph
  description: |
    Zero-shot verifier graph for few-shot evaluation using RLM strategy.
    Uses calibration examples to learn evaluation criteria, then RLM tools for massive traces.
    Handles massive traces (> 500K tokens) via tool-based search and analysis.

start_nodes: [materialize_trace]

nodes:
  materialize_trace:
    name: materialize_trace
    type: DagNode
    input_mapping: '{"trace": state.get("trace", {}), "calibration_examples": state.get("calibration_examples", [])}'
    implementation:
      type: python_function
      fn_str: |
        import json
        def materialize_trace(trace: dict, calibration_examples: list) -> dict:
            """Materialize trace and calibration examples to files for RLM processing."""
            trace_json = json.dumps(trace, indent=2)
            examples_json = json.dumps(calibration_examples, indent=2)
            return {
                "trace_content": trace_json,
                "examples_content": examples_json,
                "trace_size_chars": len(trace_json),
                "estimated_tokens": len(trace_json) // 4
            }

  rlm_evaluate:
    name: rlm_evaluate
    type: DagNode
    input_mapping: '{"query": "Evaluate this execution trace using the calibration examples. Learn patterns from examples and apply them consistently. Use tools to efficiently search and analyze the trace, then produce event_reviews, outcome_review, and event_totals.", "trace_content": state.get("trace_content", ""), "examples_content": state.get("examples_content", ""), "expected_score": state.get("expected_score"), "expected_rubric": state.get("expected_rubric", ""), "options": state.get("options", {})}'
    implementation:
      type: rlm_compute
      rlm_impl: v1
      model_name: MUST_OVERRIDE_MODEL
      max_iterations: 25
      max_root_calls: 50
      max_subcalls: 100
      is_verifier: true  # Requires structured event_reviews/outcome_review output
      tools:
        - materialize_context
        - codex_exec
        - local_grep
        - local_search
        - delegate_lm
        - add_evidence
      system_prompt: |
        You are evaluating an execution trace using few-shot calibration examples with RLM tools.

        Available tools:
        - materialize_context: Write input fields to files in sandbox. Use field_name="trace_content" for trace, field_name="examples_content" for calibration examples.
        - codex_exec: Run shell commands (grep, awk, python, etc.) to search/analyze
        - local_grep: Regex search on materialized content
        - local_search: Substring search
        - delegate_lm: Call a sub-LLM for semantic analysis with structured output
        - add_evidence: Record evidence supporting your evaluation (persists across summaries)

        Strategy:
        1. Use materialize_context with field_name="trace_content" and field_name="examples_content" to write files
        2. Use codex_exec/local_grep to search for relevant sections
        3. Use add_evidence to record specific findings that support your scores
        4. Use delegate_lm for semantic analysis on small chunks
        5. Learn evaluation patterns from calibration examples
        6. Evaluate events against learned criteria
        7. Aggregate into final outcome review

        Return JSON with event_reviews and outcome_review matching the verifier API format.

  parse_rlm_output:
    name: parse_rlm_output
    type: DagNode
    input_mapping: '{"rlm_output": state.get("rlm_evaluate_output", ""), "event_count": state.get("estimated_tokens", 0) // 1000}'
    implementation:
      type: python_function
      fn_str: |
        import json
        import re
        
        def parse_rlm_output(rlm_output: str, event_count: int) -> dict:
            """Parse RLM output into structured event_reviews and outcome_review."""
            # Try to extract JSON from RLM output
            json_match = re.search(r'\{.*"event_reviews".*"outcome_review".*\}', rlm_output, re.DOTALL)
            if json_match:
                try:
                    parsed = json.loads(json_match.group())
                    return {
                        "event_reviews": parsed.get("event_reviews", []),
                        "outcome_review": parsed.get("outcome_review", {}),
                        "event_totals": parsed.get("event_totals", [])
                    }
                except:
                    pass
            
            # Strict: create basic structure
            return {
                "event_reviews": [{"criteria": {}, "total": 0.5, "summary": "RLM evaluation"}],
                "outcome_review": {"criteria": {}, "total": 0.5, "summary": "RLM evaluation"},
                "event_totals": [0.5] * max(1, event_count)
            }

control_edges:
  materialize_trace:
    - target: rlm_evaluate
  rlm_evaluate:
    - target: parse_rlm_output
  parse_rlm_output: []

verdict_weights:
  parse_rlm_output: 1.0

aggregation_policy: weighted_average
